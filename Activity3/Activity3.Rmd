---
title: "Business Intelligence from Web Data Analytics and Data Mining using R and
  AI - Activity3"
author: "Jannik Guldmand, Sophia Klimova & Adam Dapoz"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: false
    theme: flatly
    highlight: tango
    df_print: paged
    code_folding: show
  pdf_document:
    toc: true
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
library(rpart.plot)
library(factoextra)
library(ggplot2)
```

# Activity3

## Introduction 
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam nec orci vitae ligula pretium sollicitudin. Sed sed efficitur nulla. Morbi sed mollis odio, at interdum est. In at imperdiet metus, ac placerat elit. Maecenas dictum sit amet arcu in cursus. Integer dictum elementum dui, id interdum sem consequat in. Nullam sapien ligula, elementum id nisi at, convallis sollicitudin elit. Sed luctus pulvinar lobortis. Quisque a diam in arcu vulputate scelerisque. Aliquam lobortis aliquet dui, varius imperdiet libero interdum non.

## Requirements & Goals
* Try to Predict/Classify if a player is gonna support
* Try to Predict a players Rank


# Data
```{r load data}
data = read.csv("tldata.csv")
head(data)
summary(data)
str(data)
```
```{r prparing the data}
## Preparing the data

#make some things factors
data$Country           = as.factor(data$Country)

data$Country = sub("Korea, Republic of", "Republic of Korea", data$Country)
data$Country = sub("Venezuela, Bolivarian Republic of", "Republic of Venezuela", data$Country)
data$Country = sub("Macedonia, the former Yugoslav Republic of", "Republic of Macedonia", data$Country)
data$Country           = as.factor(data$Country)

data$Rank              = factor(data$Rank, levels=c("D","D+","C-","C","C+","B-","B","B+","A-","A","A+","S-","S","S+","SS","U","X","X+"))

data$Active.This.Week = as.factor(data$Active.This.Week)
data$Active.This.Week = ifelse(data$Active.This.Week == "Yes", 1, 0)

data$Supporter.Status. = as.factor(data$Supporter.Status.)
data$Supporter.Status. = ifelse(data$Supporter.Status. == "Yes", 1, 0)

data$Wins = as.numeric(data$Wins)
data$Games.Played = as.numeric(data$Games.Played)

data$Username = as.character(data$Username)


#remove index (standing does this)
data$X = NULL
str(data)
```

```{r random seed}
set.seed(4)
```


## splitting into test and train
```{r defining data splits}
num_of_splits = 10
splits = sample( rep(1:num_of_splits, ceiling(nrow(data)/num_of_splits) ), nrow(data) )
```

```{r check for uniform-distribution}
#check for uniform-distribution
summary(as.factor(splits))
str(splits)
```

```{r splitting data}
train = data[splits!=1,]
test = data[splits==1,]

#nrow(train) + nrow(test)
#nrow(test)

#dynamically
#testdata(7)
#=> train
#=>test

```

## Normalizing data
```{r normalizing data}
data_normalized = data

for(i in c(1,4,5,6,7,8,9,10,11,12)) {
  data_normalized[,i] = scale(
                              data_normalized[,i], 
                              center=min(data_normalized[,i]), 
                              scale=max(data_normalized[,i])-min(data_normalized[,i]))
}

str(data_normalized, give.attr=F)
```


## Handling flags
```{r handling flags}
#handling flags
#turn rank factor into flags
for (i in 1:length(levels(data_normalized$Rank))) {
  data_normalized[,ncol(data_normalized)+1] = 0 #make new column
  #set appropriate ones to 1
  data_normalized[
    data_normalized$Rank == levels(data_normalized$Rank)[i], #select rows matching rank
    ncol(data_normalized)] = 1 #select last column (just added)
  
  varname = sprintf( "flag%sRank", levels(data_normalized$Rank)[i] )
  varname = sub("+", "Plus", varname, fixed=T) #fixed=T treats '+' literal
  varname = sub("-", "Minus", varname, fixed=T)
  names(data_normalized)[ ncol(data_normalized) ] = varname
}

# remove orig rank var and remove one flag
data_normalized$Rank = NULL
data_normalized$flagDrank = NULL

str(data_normalized, give.attr=F)
```
```{r}
#train
```

## Balancing the data
```{r balancing data}
#make training set with higher proportion of supporters (50/50 split)
allsupporters = data[data_normalized$Supporter.Status. == 1,]
allnonsupporters = data[data_normalized$Supporter.Status. == 0,]

allsupporters
allnonsupporters
train_balanced = rbind( allsupporters, 
               allnonsupporters[sample(1:nrow(allnonsupporters), nrow(allsupporters)),] )
#shuffle it
train_balanced = train_balanced[sample(1:nrow(train), nrow(train)),]
```

## Z-score standardizing data
```{r z-score standardizing data}
# Z-score standardization
data_standardized = data

#standardization of numeric variables, for decision trees is not necessary
data_standardized$Wins.s = (data$Wins - mean(data$Wins))/sd(data$Wins)
data_standardized$Games.Played.s = (data$Games.Played - mean(data$Games.Played))/sd(data$Games.Played)
data_standardized$Tetra.Rating.s = (data$Tetra.Rating - mean(data$Tetra.Rating))/sd(data$Tetra.Rating)

data_standardized
```


# Regession
## Linear Regression
## Multiple Regression

<hr>

# KNN (Sophie)




<hr>

# Decision Trees (Jannik)


```{r}
# Tree data
#data_without_support = data
#test_without_support = test
#train_without_support = train

#drop support
#data_without_support$Supporter.Status. = NULL
#test_without_support$Supporter.Status. = NULL
#train_without_support$Supporter.Status. = NULL

#Train: drop username, country
train_without_username = train
train_without_username$Username = NULL
train_without_username$Country = NULL

#Test: drop username, country
test_without_username = test
test_without_username$Username = NULL
test_without_username$Country = NULL

train_balanced_without_username = train_balanced
train_balanced_without_username$Username = NULL
train_balanced_without_username$Country = NULL

#train_without_username$Standing = NULL
```

```{r}
#train_without_username
train_balanced_without_username
```

## CART v1
```{r}
set.seed(1)
#cartfittrain = rpart(Supporter.Status.~., dat=train_without_username, method="class", control=rpart.control(minsplit=4, cp=0.0015))
cartfittrain = rpart(Supporter.Status.~., dat=train_balanced_without_username, method="class", control=rpart.control(minsplit=4, cp=0.003))
#cartfittrain = rpart(Supporter.Status.~ Glicko.Rating+Country, dat=train_without_username, method="class")
rpart.plot(cartfittrain, type=2)
#control=rpart.control(cp=0.0005)
#min-splits

```

## CART v2
```{r}
train_without_username
cartfittrain2 = rpart(Supporter.Status.~., dat=train_without_username[train_without_username$Standing<400,], method="class", control=rpart.control(minsplit=4, cp=0.03))

rpart.plot(cartfittrain2, type=2)
```
```{r}
str(train_without_username)
```

## CART v3
```{r}
cartfittest2 = rpart(Supporter.Status.~., dat=test_without_username[train_without_username$Standing<400,], method="class", control=rpart.control(minsplit=4, cp=0.03))

rpart.plot(cartfittest2, type=2)
```
```{r}
str(test_without_username)
```

## CART v4







<hr>
# Neural Networks (Adam)







<hr>
# Clustering (Jannik)


## preparing data for clustering
```{r}
data2 = read.csv("tldata.csv")
cluster_data = data2

#drop unnecessary columns
cluster_data$X = NULL
cluster_data$Username = NULL
cluster_data$Country = NULL
cluster_data$RankColour = NULL

#converting charecter to factors
cluster_data$Rank = as.factor(cluster_data$Rank)
cluster_data$Active.This.Week = as.factor(cluster_data$Active.This.Week)
cluster_data$Supporter.Status. = as.factor(cluster_data$Supporter.Status.)

# generate dummy-variabler using model.matrix
dummy_vars <- model.matrix(~ Rank + Active.This.Week + Supporter.Status. - 1, data = cluster_data)

# combine de numeriske data and dummy variables
numeric_data <- cluster_data[sapply(cluster_data, is.numeric)]
numeric_cluster_data <- cbind(numeric_data, dummy_vars)

str(numeric_cluster_data)
```
```{r}
numeric_cluster_data
```
```{r}
head(dummy_vars[, grepl("Rank", colnames(dummy_vars))])
```
## controling of dummyvariables was done correctly

```{r}

which(numeric_cluster_data$`RankA` == 1)
which(numeric_cluster_data$`RankA-` == 1)
which(numeric_cluster_data$`RankA+` == 1)
#which(numeric_cluster_data$`RankB+` == 1)
#which(numeric_cluster_data$`RankB-` == 1)
#which(numeric_cluster_data$`RankB+` == 1)
#which(numeric_cluster_data$`RankC` == 1)
#which(numeric_cluster_data$`RankC+` == 1)
#which(numeric_cluster_data$`RankC-` == 1)
```


```{r}
# Standardiser (skalering)
cluster_data_scaled <- scale(numeric_cluster_data)

# Find klynger med K-means (10 clusters)
kmeans_result10 <- kmeans(cluster_data_scaled, centers = 10)

# found clusters
kmeans_result10$cluster
```
```{r}
# Plot using PCA
pca <- prcomp(cluster_data_scaled)
pca_data <- data.frame(pca$x[, 1:2], Cluster = factor(kmeans_result10$cluster))
ggplot(pca_data, aes(PC1, PC2, color = Cluster)) +
  geom_point() +
  ggtitle("K-means clustering på shoppers datasæt")
```
## Elbow Plot
```{r}
# Elbow methode using fviz_nbclust to plot total  within-cluster sum of squares (WSS)
fviz_nbclust(cluster_data_scaled, kmeans, method = "wss") + labs(subtitle = "Elbow method")
```
```{r}
library(parallel)

# Use all available cores
num_cores <- detectCores() - 1

# Sample data
sampled_data <- cluster_data_scaled[sample(1:nrow(cluster_data_scaled), 3000), ]

# Custom WSS function using parallel
wss_parallel <- mclapply(1:10, function(k) {
  kmeans(sampled_data, centers = k, nstart = 10)$tot.withinss
}, mc.cores = num_cores)

# Plot elbow manually
plot(1:10, unlist(wss_parallel), type = "b",
     xlab = "Number of Clusters K",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method (Parallel)")

```
```{r}
library(parallel)

# Use all available cores
num_cores <- detectCores() - 1

# Use full dataset (no sampling)
full_data <- cluster_data_scaled

# Custom WSS function using parallel
wss_parallel <- mclapply(1:10, function(k) {
  kmeans(full_data, centers = k, nstart = 10)$tot.withinss
}, mc.cores = num_cores)

# Plot elbow manually
plot(1:10, unlist(wss_parallel), type = "b",
     xlab = "Number of Clusters K",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method (Full Dataset, Parallel)")
```
```{r}
# Find klynger med K-means (4 clusters)
kmeans_result4 <- kmeans(cluster_data_scaled, centers = 4)
```

```{r}
# Plot using PCA
pca <- prcomp(cluster_data_scaled)
pca_data <- data.frame(pca$x[, 1:2], Cluster = factor(kmeans_result4$cluster))
ggplot(pca_data, aes(PC1, PC2, color = Cluster)) +
  geom_point() +
  ggtitle("K-means clustering på shoppers datasæt")
```
## 3D Plotting the 4 Clusters

```{r}
library(plotly)

# PCA på de skalerede data
pca <- prcomp(cluster_data_scaled)

# Lav et data frame med de første tre komponenter + klynge labels
pca_data <- data.frame(PC1 = pca$x[, 1],
                       PC2 = pca$x[, 2],
                       PC3 = pca$x[, 3],
                       Cluster = factor(kmeans_result4$cluster))  # 4 clusters

# 3D plot med plotly
plot_ly(pca_data, 
        x = ~PC1, y = ~PC2, z = ~PC3, 
        color = ~Cluster, 
        colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"),  # fire farver
        type = "scatter3d", 
        mode = "markers") %>%
  layout(title = "3D K-means clustering med 4 klynger (PCA)")

```


```{r}
summary(pca)
```

```{r}
pca$rotation
```

